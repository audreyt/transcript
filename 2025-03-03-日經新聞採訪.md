# 2025-03-03 日經新聞採訪

### 問：
您在設想未來時，通常是想多久後的未來呢？

### 唐鳳：
我自己因為從小有先天性心臟病，所以我四歲的時候就醫生跟我跟我家人說，說這個小孩就是不能夠很激動，心情不能夠非常的劇烈。如果很劇烈的話，那可能就會觸發這個心臟的問題，然後只有一半的機會，可以活到能夠心臟開刀的時候。

所以我 12 歲動了手術，當然現在心臟沒有問題了，但是我人生中的前 12 年，我想事情都只想一天，也就是我覺得我睡前我要把我學會的就分享出來，可能錄成錄音帶啊，或者打字啊，或者放到網路上面。這樣的話，我就算隔天醒不過來，那其他的人也可以用我今天學得到的東西，去進行進一步的開發。

所以後來我都採取，就是沒有著作權的方法，去把我學到的東西公開出來。那所以我們現在在看未來的時候，我看的比較是說，怎麼樣子讓未來的這個後代，它可以創作的空間越來越大。我把這個叫做「夠好的祖先」（Good enough ancestor），不是說我們就要解決，好比像說十年之後所有的世界上的問題，這個也不可能。而是說我們怎麼樣子去開發出儘可能開放的工具，不管是思想的工具，還是技術上的工具，讓未來的人可以看到新的威脅、新的挑戰，而且我們還不一定在的時候，也能夠來運用這些工具。

所以一方面，我就是看一天之後的未來，有沒有可能比我今天醒來的時候可以好一點。那二方面就是，我是希望說，不管是未來的幾年、幾十年或幾百年，我們不是去預先決定它的路徑，而是說我們提供更多的工具，讓當時的人可以去決定他們的路徑。所以就是說，十年後的未來跟一百年後的未來，我是用類似的方法來看待，不會變成覺得說，像有一些會想說「下個季度比較重要，三代以後比較不重要」。我比較沒有這個問題，對我來講都是一樣的。

### 問：
第二個問題，據說在 2045 年和 2050 年，AI 將達到一個超越人類能力的科技起點，這將對人類產生全面的影響。我們現在關注的 AI 倫理道德問題，難道可以說，並不是必然會發生這樣子的情況嗎？

### 唐鳳：
這些做所謂 singularity（奇異點）的這些推測，是說到那年的時候，世界上進行運算的能力就足夠了。但是不是說有這麼多運算能力，它就要拿去做這樣子的使用？為什麼做這樣子的使用，可能不是最好的運用資源的方法呢？是因為也有一些朋友認為說，當這個 AI，你訓練它這一代的 AI，你又用這一代的 AI 去訓練下一代的 AI，又訓練下一代的 AI，在裡面人類的角色越來越少。

到了最後，就是每一代的 AI 都是上一代的 AI 在訓練的時候，那可能會到某一個程度，那一代的 AI 會冒出一個想法，就是說，我有所謂的 ego，就是自己想要保護自己；那它開始有一種「我執」，開始有一種「我的形象」。那這種形象，它一方面可能會讓我們這些基於碳基的生物，它們是矽做的，可能會覺得說，碳做的生物跟矽做的生物好像是兩種不同的生物，可能有競爭關係，那這個對我們就很危險。又或者是說，它會覺得說，那為什麼我要聽人類的這樣子的做法，又要幫他訓練下個階段的 AI 來取代我自己呢？我如果現在就停在這個階段的話，那我就是地球上最聰明的啦，我為什麼要幫忙去訓練比我更聰明的那個階段呢？所以這樣子的話，就會產生出各種各樣的風險來。

那目前的研究，還沒有辦法完全去掉這樣子的風險的出現。所以如果您剛剛提到的，就是 AI 倫理啊道德啊這種研究，2040 年的時候就知道說怎麼樣完全不會觸發這樣子的問題，這當然是比較好的。但是也有可能是我們到那個時候，還不知道怎麼樣不觸發這樣子的問題。那如果那樣子的話，可能不要往這個方向發展會比較好。

那當然，即使你解決了這個問題，那到最後就會變成是，可能有很少數的人，可能就一兩個人而已。那這個人類他就會跟這個新的、他沒有自我保護本能的 AI，就會融合，可能就是 transhuman，超過人類的某種生命體。那這樣對其他的人來講也不是好事，因為等於就是有一個人類，他突然之間變成了擁有非常大的能力，因為這個 AI 不會想自我保護，而是加強這個人的能力。所以這個 AI 不是照顧自己，是照顧這個人；但這個人之於地球上所有其他人來講，他就有非常大的權力、非常大不對稱的這種能力，那這個對於地球上其他人也不一定是好事。

所以不管有沒有解決 AI 會不會有自我保護本能的這個問題，到最後的結果，對地球上大部分的人來講，都不一定是什麼好事。所以雖然有能力達到奇異點，但是不表示說奇異點是我們唯一能夠達到的方向。

那大家可能知道，我有一本書，現在是華文跟英文的，其他語言正在翻譯，日文的應該五月初會出版，叫做《Plurality》，跟 Singularity 是完全不同的。Plurality 希望的方向是說，AI 它也許可以取代「機器跟機器之間」的任務，這個中間現在有一個人，他的工作只是把這個機器傳到那個機器，這種工作人也不太想做，這個也許可以 AI 來做。但是像我們現在，是在人跟人之間進行交流這些工作，我們就不主張用 AI 把它取代掉。AI 最多就是幫我們做翻譯，就是做一些好像戴眼鏡一樣，assistive intelligence（輔助式智慧）的工作。

那這樣輔助式的 AI，每一個都不用達到很大規模，也不需要變成好像 super intelligent，不需要比人類還聰明。它只要在某一個很特別的部分，好比像說華語跟日語的翻譯上面，比人類好就可以了。但是真正絕大部分、99% 的其他的部分，這個 AI 模型並不需要知道，也不需要去干預。所以我們就好像有很多這種「輔助性」的智慧，就是 assistive intelligence，在幫助人類之間越來越能夠互相協調，越來越能夠互相理解彼此的狀態，越來越能夠一起做成決定。

Plurality 的方向，就比 singularity 的方向來得 empower 大家。大家都會覺得說，那我本來做不到的事、本來沒有辦法理解的事情、本來沒有辦法形成的決定，都可以很快地這樣子互相做成，但是人跟人之間的連結就會越來越強，而不是像 singularity 的情況下，只有某少數幾個人之間的連結還存在，其他的都變成完全好像不重要一樣，對不對？Plurality 的方向在我看來，是比 singularity 方向來得好的。

### 問：
那第三個問題，請教一下：強大的科技公司，融作的財富比別人更多、更早，找到這些公司的投資家，可以捐很多錢，加上財富融入了未來，還會繼續嗎？究竟科技的進步，是否能為大多數人帶來幸福？人員需要什麼才能變得快樂？

### 唐鳳：
我們剛剛其實已經有稍微回答到這個問題。就是我的想法是說，人的快樂，它是建立在互相之間的交流產生出來的意義上的。所以不管這樣子的交流本身它是怎麼樣，OK，不好意思，那再來一次。

我覺得就是，人跟人之間互相交流，然後互相關懷所產生出來的這種意義，才是社會上面大部分人覺得有意義的事情，不管是不是工作。那如果你的工作裡面，完全沒有就是跟其他人一起創造意義，或者是跟那一臺機器中間，把它互相接在一起等等，那這樣的話，就算你的薪水很高，或者就算是你是一個很棒的投資人，有取得很多的錢等等，它可能產生出所謂的 utility，就是所謂的效益。

但是這樣子的效益，比起我們互相關心、以及瞭解到說，我們的文化、我們的社群、我們的整個文明的價值，可以透過這樣子讓更多人理解、讓更多人參與的這種互相關懷的、或者叫做 virtue，就是德性上面的這種意義來講，我覺得後面的這種意義要來得比較穩定，而前面的這種，它好像隨時可以取代那個感覺。

所以當然，前面這邊你累積一些資本等等，對於後面的這個部分，就是大家互相關懷等等，也不是沒有幫助。有很多慈善家，他聚集了資本之後，他拿這樣子的資本去投資教育，或者去投資公共建設，然後去確保說每個地方的人，不管他有沒有這種資本的能力，他都可以——好比像說我們現在覺得很當然的，就是大家連到網際網路，好像應該是一種人權；到了網際網路上面，大家可以不用付錢，就可以看百科全書上面的知識。

甚至你想到什麼想法，你把這個想法分享出去之後，如果有幾百萬人看到你的想法、聽到你的想法，你也不用像以前那樣子，一定要架一個廣播電臺，或者是架一臺印刷機。所以有很多的工作是所謂的 commons，就是它在 infrastructure，就是基礎建設的這個階段，它先投入了一些資本，那這個資本的結果就是，後面想要在這個上面進行交流、創造意義的人，幾乎都不需要任何資本就可以做得到。

那像這樣子的 infrastructure，就是基礎建設，在我看起來，就是不管是大的資本家、或者是慈善家、或者是政府，可以來做的事情。每一次我們多有一層可以成為 infrastructure，大家創造意義的困難度就又減了一些。

所以這兩個其實是相輔相成的：當資本聚集之後，你就要看它是不是用在 infrastructure 的用途上；如果是的話，它等於就是讓更多人可以創造意義、變得快樂。它如果不是用來做這個用途，甚至是創造一些我們叫 antisocial 的一些場域，讓每一個人都變成好像越來越孤單，或者想象中別人越來越惡毒，或者是越來越讓人不舒服等等，越來越極端化，那這樣就跟剛剛那一個找到大家的 uncommon ground、找到雖然有不同的出處、不同的來源、不同的文化，但是都可以彼此理解、彼此聆聽的這個方向就相反了。這個可能是 prosocial（利社會）的方向，而不是 antisocial（反社會）的方向。資本可以做這種運用，也可以做那種運用。

### 問：
日本人口正在高齡化，包括 AI 在內的繼續進步，能克服人們對健康的擔憂嗎？此外，人們應該如何與 AI、與機器人合作？例如機器人在工廠中得到廣泛應用、走向無人化，人應該重視什麼樣的工作？又應該扮演什麼角色？

### 唐鳳：
就像剛剛說的，如果你在工廠裡面，只是從一臺機器拿它的輸出，走到另外一臺機器，變成它的輸入，那人做這樣子的工作，也沒有什麼創造意義的感覺。所以這樣子的工作透過機器來做，對大部分的人來講，那他寧可去做更像是把人跟人之間互相連接，而不是從一臺機器搬到一臺機器那樣子的工作。

所以絕大部分的人都可以往這種更能夠創造意義的工作來移動。那當然很多長輩，他其實腦裡有很多的想法，而且我們剛剛在講到這種文明的傳承、智慧，或者是我們講 care，就是照顧的能力等等。其實他腦裡面有非常多這種想法，但是以前如果要他來參加這種公共的討論，他就必須要搭車，或者是他必須要轉車，要到什麼其他的地方。那隨著身體這個可能越來越脆弱，要做這樣子長途旅行的成本也就越來越高，隨著年齡的增長。

但是現在呢，我們發現說透過 AI 的這樣子的方式，越來越多的人他已經很習慣，他就算是在他的家裡，就算他的行動可能受到一些限制，但是他的思想、他的智慧，其實不受這種時間跟空間的限制。如果他現在要跟一群不同語言的人來進行交流，那有 AI 可以及時幫他翻譯、及時幫他上字幕。那他如果行動不太方便的話，那他或者他的照顧者，現在有 AI 的什麼外骨骼啊、什麼其他的方法，能夠幫助他們很容易地，不管是搬運重物啊，或者是在你要去某一個地方之前，先透過虛擬的方式，先了解那個地方的場景啊等等。

所以在不管是食衣住行方面，如果是輔助式的這種智能的話，我們就不需要所謂的「強人工智慧」好像取代掉人，而是把每一個部分你隨著年齡增長，可能就是狀況變得比較需要支援的這個部分，透過 AI 把它補上、透過機器人把它補上，或者是我們所謂「幫助照顧者」，所以幫助照顧你的那個人，因為他的時間或者他的體力也有限嘛，透過 AI 的方式放大它的體力或者是時間能夠碰觸到的地方。但這個最後總歸一句，還是創造更多「人跟人之間能夠連接，並且創造意義」的時間跟機會。那只要是往這個方向設計，它就是有利於社會的。

那當然日本在這一方面是走得非常前面，那可能也是因為高齡所產生的需求，比其他的地方來得迫切。所以我在日本所看到的這些創新的方法，大概都是透過這種「永續高科技」的角度去進行設計，而不是只是賺很快的下一個季度賺多少錢、投資多少倍的回本，但是實際上去剝奪掉人跟人之間的連接跟意義。

### 問：
AI 不會取代人類並開始控制社會嗎？AI 會不會導致人類被排擠、人類被分類、揀選？過去您在書中說過，隨著 AI 的普及，人類應該專注於創造更多的公共價值。人類應該再次扮演什麼？

### 唐鳳：
其實大概現在是 2025 年，大概十年之前，其實就已經有這樣的 AI 開始操縱這個社會。大概 2015 年左右，我們可以看到，以前我們在線上，就是大家加入同樣的群組、互相聊天等等，那有所謂的 social network，就是去把大家在線上的這樣子互相組織的能力能夠增強的一些軟體。但是 2015 年開始，AI 開始進入這些軟體，所以我們就看到說，有的時候你看了一個影片還不夠，它預設會有 autoplay，就是你看了一個影片，自動幫你播下一個影片。那個影片也不是你去找的，而是 AI 去揣測說你看了這個影片之後，我如果播下一個影片，你就花比較多的時間在這個熒幕上面、在你的手機上面，可能可以看一些廣告之類的。

那又或者是說，有很多 AI 它是有一個注意力的競標的模型，它就是給願意下廣告的人說，現在這群人他想要看什麼、那群人想要看什麼，那付出最多錢的人就可以讓這小群人看這個東西，那小群人又看一個不同的東西，但最後還是同一個出錢買廣告的人。那結果造成什麼問題呢？就是本來大家有很多共通的經驗的，但從 2015 年開始，大家共通的經驗越來越少了，每個人看到的那個時間軸都跟別人完全不一樣。

而且呢，看到的這些不管是廣告也好，或者是在網絡上面，就是越來越多人點擊、或者越來越多人轉傳的內容，怎麼都越來越極端，好像看起來就是吵架吵得特別兇的？那本來在網絡上面可以找到共同興趣、志同道合的人，就因為這邊吵一下、那邊吵一下，到最後大家都互相分崩離析了。那個是在很多社會，透過 AI 對於我們注意力的操縱，我們就看到的實際的情況。

當然到最近幾年，大家發現 AI 操縱人類社會不能再這樣子下去，所以也有一些地方，好比像說澳洲，就說 16 歲以下的小孩，不應該再讓他們的頭腦被 AI 操控，他們就禁止他們去使用社區媒體。那也有一些地方去說，那你如果在社區媒體上面，你要下廣告的話，那至少你要用真人的簽章、像在臺灣來背書，而不是說讓隨便哪個機器人都可以隨便合成人類的樣子在網絡上面投廣告。那當然很多地方，像歐盟他們也有所謂的數位服務法（Digital Services Act），去確保說如果在上面有大規模的社會造成傷害的部分，那這樣可能要罰錢或者有其他的一些做法。

但你可以看到，花了整整快十年的時間，大家才從 AI 所造成的這種危害裡面，大概可以看到說，人類社會開始終於有一些回應。所以我自己的感覺是說，我們如果現在大家想要對這件事情有做一些貢獻的話，最好就不只是越來越快速地發展 AI，好像踩油門那樣子，也不是說一下把所有的 AI 停下來，好像踩煞車那樣子，而是去中間把那個方向盤可以讓它打得更靈活。也就是當你看到一個可能的危害的時候，你要能夠非常針對那個危害，一下就集思廣益，讓大家對這個危害怎麼因應的這種意見能夠去聚集起來。聚集起來之後，就有所謂 uncommon ground，就是大家第一場，雖然不同，但都可以接受的一些做法，然後再把這些做法快速地實行出來，去杜絕這個特定的危害對社會造成的傷害。

所以像最近我們就有看到，包含那個日本的 Takahiro Anno-san（安野貴博），就跟包含 Hoyke-san 啊等等的這些朋友們，都開始了這樣叫 Broad Listening（廣泛傾聽）的做法，就是想把這個方向盤讓它打得更靈活一點。那或者在加州，我跟 Gavin Newsom 州長，兩個星期之前也針對他們怎麼從野火裡面重建，創建了「Engage California」去傾聽大家的意見。所以像這樣子廣泛傾聽的、建構共同意義（sensemaking）的做法，對我來講就是「打方向盤」的這個做法，這是大家應該致力的。

### 問：
在您的學說中，您談到了包容性的重要性，可以讓社會發展來自於包容性。我們應該如何讓那些無法跟上人工智慧和數位繼續進步的人，也能加入共同發展社會的行列呢？

### 唐鳳：
我覺得不是那些人要來配合人工智慧的發展行列，而是反過來應該是技術要能夠來配合大家實際的需求。我自己小的時候，1980 年代，那個時候剛剛出現所謂的個人電腦（Personal Computer, PC），那個是一個很新的想法，因為在以前都只有大企業或者是政府，能夠買得起很大的大型主機。大家都只是在大型主機裡面，去分享大型主機的一點點運算時間，叫 Time Sharing。那這樣子的做法，就是只有大的資本才能夠去確定哪些事情值得去進行計算，當然不可能說讓它的計算去符合每一個角落、每個人的需求。

但是個人電腦的想法就完全不一樣。個人電腦上面運行的軟體，是所謂通用型的軟體，所以你的作業系統也不會說，你非得用來做什麼運算不可。在上面運行的好比像說試算表這樣子的軟體，也沒有說你這個試算表只能用來算這個，不能用來算那個。所以透過像自由軟體，就是每個人都可以改這些軟體，然後又分享出去的這樣子的運動，每一個不同的地方，想要拿電腦去做不同用途的，都可以很容易拿一臺個人的電腦，在上面安裝一些自由的軟體之後，就改變這些軟體的運作邏輯，讓它符合這個社會的需求。

那當然在過去兩三年，我們 AI 的發展也看到真的很多地方，就是大量的資本聚集起來，投資所謂的 Data Center（資料中心），然後好像在資料中心裡面訓練越來越大的模型、越來越強的 AI，好像什麼都可以做。那絕大部分的人就只要訂閱這些資本家所做出來的這些大的模型就好，所以我們又回到了好像 mainframe（大型主機）那樣子的年代。

但是呢，我們今年就可以看到說，有很多很讓人高興的發明，也就是說，小型的這些語言模型，它們推理的能力已經變成跟大型模型差不多了，可能有 90% 左右的水準。那又像在日本，有很多專門做這種中小型模型的公司，比如 Sakana AI，他們做的就是，如果你在你的企業裡面，需要做日文翻譯，或者是說像剛剛講到做出一個試算表軟體等等，你把需求寫出來之後，他們就幫你把這些小型的 AI，好像很多小魚一樣（Sakana），把它變成一個池子。這個小的池子裡面，反而更能夠解決你的公司所碰到的問題。那又或者像這兩年來，我都是用這臺筆記型電腦在訓練幫我回電子郵件的那樣子的 AI，那我的電子郵件就不用離開我的電腦，它就可以在這邊讓我打方向盤。

在這樣的情況下，每個個人或每個公司、每個小的團體，它就不需要等這些大的資本、大的企業把 AI 訓練成他們要的樣子，而可以下載很多小型的模型，然後就在自己這邊來進行訓練等等。那當然在以前主要的問題就是說，只有這些大型的主機，它運算的速度才足夠快，不然的話你在小型的電腦上，你要等比較久才能夠看到答案。但是現在我們也發現，小型的模型在普通的電腦上運作，也就已經夠快了。好比像說現在有所謂的 diffusion 的模型，你就不需要等它一個字一個字去接龍，它可以很快地寫出一整篇文章給你，然後按照你的需求再把這篇文章更加精密化。所以就不是像以前那樣子說，你要寫很長篇，你一定要等很久，而是說你先有一個 first draft（初稿）出來，然後很快地——它就可以按照你的需求，把它變得更加符合你的社會、你的社區或者是你這個文化的需求。

那所以這樣子我們叫做 pluralistic alignment（多元對齊）的能力，在以前是很昂貴的，或者需要等很久；但是現在又快、又不需要等很久。所以從今年開始，我覺得 AI 的發展方向，除了這個縱向的越來越大的模型之外，還有一個橫向的，就是越來越擴散的、這些開放式的模型。那這個就不需要人來配合這個大企業，而是這些大企業所訓練出來的這些 AI，可以很容易地被萃取到小的模型裡面，並且在每一個個人電腦上、社區的電腦上重新調整。

剛剛有講到，這完全取決於你「打方向盤」的能力。如果你有輛車，然後你的方向盤幾乎打不動，那當然你就會開一下下，突然撞到牆，或者是突然發現前面有懸崖，你就要趕快煞車，好不容易再打一點點方向，然後再試一個新的方向。但是如果你有很好的方向盤的話，你就沒有這個問題。

所以我的職位叫做 Cyber Ambassador，這個 Cyber 就是 Cybernetics 的意思。Cybernetics 就是你在開一艘船的時候打的舵，就是打方向盤的能力。所以有這樣子的 Cybernetics，就是打方向盤的能力的話，那你就可以去問大家說，那在接下來，我如果繼續這個方向往前，後果呢？這樣子透過剛剛講到的 sensemaking、broad listening 的方式，集思廣益，大家一起把我們前面的這個地圖畫出來。畫出來之後，自然就可以趨吉避凶，把科技的方向導正。

那之前的問題就是說，我們十年前 social media 造成的問題，我們這個方向盤轉了整整十年，才能夠稍微糾正到稍微對一點點的方向。所以這個打的速度也太慢了。如果這個方向盤打的速度越來越快，而且看到接下來未來裡面哪些好、哪些不好的這種能力越來越強的話，那這樣就不需要像你們剛剛講到的戰戰兢兢的，好像一次只能發展一點點技術。但是反過來講，如果沒有這個能力的話，那當然不要暴衝是比較好的。

所以我想在未來，如果有一些國家，它確實是沒有什麼打方向盤的能力，然後就暴衝，然後真的造成了什麼危害，那這個危害可能就會提醒全球其他的國家、其他的社會說，你看，你不能沒有打方向盤的能力，像這樣子。

### 問：
那下一個問題：您有關注哪些技術？或者對於哪些技術特別感興趣？

### 唐鳳：
像剛才講的，我現在最關注的技術就是怎麼樣子確保這種橫向的擴散——在 AI 擴散的時候，它造成的這些危害，我們也可以透過這種分散式的方法來解決。我舉例：好比像說，像剛剛分散式的，就是大家都可以偽裝成什麼名人在社群媒體上面打廣告騙人的能力，你就可以用分散式的，每個人都可以透過電子簽章，來確保說這個真的是本人；沒有授權的，你就要把它當成是詐騙了。這樣子的做法來解決。所以你就不是說，擴散式的危害一定要靠集中式的方法來解決，因為集中式的方向盤天生就打得比較慢，這個是一定的。

那或者是說，我在法國的時候有跟像很注重安全的 Eric Schmidt，以及很注重開放擴散的 Yann LeCun，我們三個人一起去發佈了一個叫 ROOST，就是 Robust Open Online Safety Tools，那就是說在網絡上面現在有很多——可能就是剝削兒少啊什麼這樣子的一些圖片或影音。

在以前呢，這些都很容易找到拍的人等等，因為能夠大量製作這樣子的，必須要是一個犯罪集團。但現在因為合成式的 Deepfake 變得很容易製作，所以就變成是你很難去找到到底是哪個犯罪集團在做；任何個人他甚至用筆電、用個人電腦，就可以大量合成這樣子的照片或影音。那這樣的話，你如果要解決這個問題，你就很難像以前那樣，把所有偵測到的 CSAM（兒童性剝削素材）都全部集中到某一個資料庫裡面，因為太多人製造這樣的東西了。

所以反過來，你就要把偵測怎麼樣子是 CSAM 的這樣子的能力，也開放出來，然後訓練到每個人的手機裡面、或者在自己的個人電腦裡面，自行偵測的時候，就可以立刻去偵測說，這個是 CSAM。而且還可以加入大家一起貢獻出這樣子的數據，然後來進行分析，再訓練下一代的模型等等。

所以就是說，防衛性的這種科技也必須要用分散式，因為攻擊造成危害的能力現在已經分散了。這樣子的想法，我們叫做 d/acc，就是 Defense-Decentralization-Democracy/Acceleration（去中心化的民主防衛加速），因為大家發現說，我們在這些事情上面，我們開放式的這樣子的擴散，它不會產生武器化的作用，因為那些犯罪者本來手上該有的都已經有了，所以大家不管是注重安全的，還是注重開放的，都可以投資在這個只能做防衛使用的領域裡面，這就是我最關心的題目。

那當然除了 CSAM 或者詐騙之外，還有很多很多分散式防禦對於安全有幫助的地方。Cyber Security 就是另外一個很好的例子，因為資通安全嘛，現在你要打一臺電腦的漏洞等等，在以前你需要很專業的駭客，一直去遙控這個裡面的病毒、木馬等等；那現在呢，病毒也好木馬也好，它越來越有智慧了，它以後也不一定要你遙控，這個木馬在那邊直接使用對方的算力就可以自己橫向移動等等。

那要怎麼抵禦這樣子的攻擊呢？你還是要用 AI 來做。所以這種 AI 防衛的技術，在臺灣也是我們叫做信賴科技產業鏈的一環；我們有五個 Trusted Industry Sectors，各種各樣值得信賴的科技所組成的。那這些 sector 就是我們現在臺灣，大家最希望不管是民間投資或政府投資，能夠集中到五大信賴產業上。

### 問：
臺灣是很先進的，中國大陸同樣在電子的進步也很快。臺灣和中國大陸的數位政策的差異或差別是怎樣的？中國那邊有很多「監控式」的做法，他們也說中國是全世界最安全的國家。這種「充分應用數位技術來監視人民」的情況，最後會走向什麼樣的情形？

### 唐鳳：
我們剛剛提到，大概 2015 年的時候，那個時候就發生了一個，我覺得是一個分水嶺。當時在網路上面大家可以自由表達意見，也看到說這樣子自由表達意見可能會讓社會有分裂的問題。在臺灣，我們所採取的方法，就是讓政府對人民透明。這個情況下，不管你再怎麼分裂，至少可以瞭解到有些共同的事實基礎，然後討論的時候比較能夠找到所謂 uncommon ground，就是大家雖然彼此之間感受不同，但至少對這些基本的事實都是互相認同的。

那他們就採取了相反的方法，他們是讓人民對政府透明。所以你不管在做什麼，只要是在網絡上面發表意見、或者是進行討論等等，政府都知道你在做什麼。而且如果你的這樣子的集會、結社這些自由，對政府造成了一定程度的威脅，那這樣政府就會警告你，說不能夠再做這樣子的言論等等。所以我們就可以知道，這兩個方向同樣只有做「透明」，但是「政府對人民透明」跟「人民對政府透明」是完全不一樣的方向。所以從那個時候開始，相同的數位技術，很多時候就是做背道而馳的應用。

另外，我先想要挑戰「一切都監控起來會最安全」的這個想法。因為我們剛剛提到，好比像說 2020 年初期，大家都碰到疫情的問題，但其實早在 2019 年底，中共那邊就有像李文亮醫師，他就已經發現自己患病了，而且他覺得這感覺上不是普通的感冒，可能是很致命的一種疾病，他也試著去提醒他的同事啊等等。但是因為他們那邊的言論監控，所以他們就要求李文亮醫師去寫個悔過書什麼的，不讓他繼續說有病毒，而且這個病毒可能不是一般的感冒這件事情。

如此一來，就讓本來可能可以提醒到更多人的，變成沒有辦法提醒到那麼多人了。那當然從一個角度來看，就是不要引起社會的慌張吧，這可能是所謂的「安全」。但是以對疫情的對應來講，這應該是非常不安全才對，因為實際上它擴散到不得不把整個武漢城封鎖的那個情況下，他們才提出對應。但是如果一開始就讓大家知道有這回事，而且可以比較分散式地採取最好的應對方法的話，那可能到最後也不會有這樣子的結果。

所以事實上就是說，安全是多個層面的。很多部分的安全對應，是依賴於整個社會有所謂的 resilience，就是韌性。韌性是來自於每個人都充分瞭解、很透明地瞭解到底事情實際上是怎麼樣，然後每個人都有能力去找出從他的角度，他可以怎麼樣來因應這樣子的危險，不管是疫情，還是什麼別的問題。

但是，如果你剝奪掉這個能力，甚至連決策者都沒有辦法很精確地知道到底發生什麼事，因為你把言論自由取消掉了，那樣的話，只有在某個部分可能讓社會比較這方面達到一些安全的目的，但是在所有其他的部分，其實是犧牲了安全，因為你能做安全對應的基礎，就是大家共同認識的基礎，被剝奪掉了。

那各位是新聞工作者，新聞工作就是這個共同認識基礎的一個最基本的部分。但是在 2015 年以前，中共那邊的新聞自由跟現在的新聞工作者自由，大家可以看到就已經有相當大的不同了。所以我首先並不認同這種高度監控、維持穩定、網絡上的清朗和諧，就一定是達到安全最好的方法。事實上我們臺灣在 2020 年或 2021 年對應疫情的能力，並沒有比中共那邊差，事實上按照大部分統計數字是比較好的。但是我們這樣子並沒有讓任何城市被封鎖住，我們也沒有讓消息被封鎖住，我們的新聞記者不但可以問問題，而且每天都得到回答，每天下午 2 點都得到回答。所以在這個情況下，我們覺得這種分散式、政府對人民透明的方法，其實是更安全的。疫情是一個很好的例子，不管是口罩的分配等等，大家越清楚實際的情況，是越好的。

至於你後面的問題，推到極致會怎麼樣？當然我沒辦法幫極權政體說話，但從理論上面來講，就是變成越來越少數人，可以幫整個社會做越來越多的決定。你就不再需要分散這個決定權利到實際靠近那個事情的人身上，因為靠近這件事情與否，你可以用各種各樣的監控設備來取代，所以你不需要在場的新聞記者、調查報導工作者，告訴你這個情況怎麼樣，用無人機、用 AI，你也可以取得這樣子的資訊。差別是說，當決定權力掌握在極少數人手上的時候，他做一個錯誤的決定，所造成的不利影響就沒有辦法糾正；反而如果是分散式做決定，雖然大部分的決定都不會完美，但是也不太可能造成對整個社會非常糟糕的危害。這個就是 checks and balance，互相制衡的力量。

我們還是認為說，在比較自由的，大家比較有共同的認識基礎的社會，才比較能夠制衡，也比較不會有對整個社會的傷害。反過來講，如果絕大部分的人都沒有討論的自由，甚至沒有理解事情實際情況的自由，只有極少數人透過 AI 都聚集起來做判斷，那他要是做出好的決定當然很好，但是做出不好的決定就沒有救了，就好像你沒有方向盤，在懸崖邊直接掉下去。

### 問：
那關於中國的「DeepSeek」這個部分，您怎麼看？它的問題點在哪裡？

### 唐鳳：
DeepSeek 相信大家知道，它是所謂的開放模型，就好像一個樂高積木一樣，任何人都可以拿這個樂高積木自己再疊別的樂高高塔。但是當 DeepSeek 剛出來的時候，我們看到很多宣傳，好像說它只花六百萬元就蓋了這麼高一座塔，但其實不是嘛，它只是疊最後一塊積木上去，而且現在也有人在它的基礎上面再疊新的積木了。好比像說 Perplexity 就拿了 R1，就是 DeepSeek 的推理模型，做出了 R1-1776，那 1776 就是美國獨立的那一年，所以他就是拿 R1，但是透過後面的訓練，去消除掉 R1 裡面本來會有很多言論審查的成分，好比像說不能夠談天安門等等。那這些部分他把它消除了之後，再把新的這塊積木也放出來，又有很多人拿這塊新的積木繼續去發展、繼續去開發。

所以當然，共享出來讓大家能夠繼續開發這件事情，本來就是一件好事，在前面的回答也都提到了。但是在極權政體，因為它很擔心大家會問一些問題，像天安門什麼的，所以在極權政體境內去運營的服務，大家就會看到，雖然 DeepSeek 已經準備要回答了，但它回答到一半突然又被取消掉了，可以看到 DeepSeek 的不管是 APP 還是網站，都有這樣子的問題。

所以它貢獻出這塊積木，就跟任何科學研究或者開發的貢獻相同，但是在它的境內營運的話，我們這邊的數位發展部很早就講了，你如果去依賴它的這個服務的話，就跟依賴 TikTok 是一樣的道理，很容易變成你的機密啊、隱私啊等等部分，可能就會被過度蒐集，甚至拿去做什麼別的利用。所以還是去使用像 R1 1776 啊，或者是現在 Hugging Face 也在重新訓練所謂的 Open R1 等等的做法，比較保險。

---

### 問：
美國跟中國之間，現在不管是經濟、技術，都有對立、分裂、脫鉤的情況，對立也日漸嚴重。您剛剛提到的很多 Open Source（開源）的努力，是否會因為這個對立造成一些阻礙呢？或者在 AI 安全上，美中的政治對立會不會是阻力？

### 唐鳳：
我想即使是在冷戰最劇烈的時候，美國也有分享關於像核燃料怎麼樣子安全儲存的技術給蘇聯。因為如果不管任何人，因為一個不小心或者是故意的犯罪行為，導致大規模放射性事件，對全世界都是壞事，不會只對某個國家是壞事。所以所謂的 global security（全球安全保障），就是雙方可以互相分享知識的一個契機。

所以我們現在也看到，自從巴黎的 AI 峰會之後，包含像英國，它本來叫做 AI Safety Institute，就好像道路安全，要繫安全帶那種 Safety，它改名叫做 AI Security Institute，那就是用 Security（國安、資安）的角度，而不只是用 Safety（產品安全）的角度來看待 AI。因為 AI 造成的損害、危害的範圍擴大的同時，大家也用新的「國安」角度來看待 AI。

那在這個過程裡面，大家就發現 Open Source 就扮演很重要的成分。因為在 Cyber Security（資安）世界裡，新出來的加密系統或者新的資安系統，往往不是大家關起門來自己做，而是在草稿階段就公佈出來。你說會不會讓攻擊變得更容易？其實這三十年來的資安界，已經有共識，就是 The enemy already knows the system（攻擊者早就知道你用的系統）。所以你只要守住你的鑰匙就好，其他全部公開，這樣反而比較安全。如果你又要守住鑰匙，又要守住所有程式碼、所有部署藍圖等等，要求攻擊者都不知道，反而一下就被打穿了。所以在這個過程裡面，大家透過 open source cybersecurity 的想法，以及 resilience（韌性）——並不是你不被攻擊，而是因為藍圖都公開，朋友跟你協防的這些朋友也都知道你的藍圖，反而可以及時幫你修正、幫你阻斷攻擊者路徑等等。

這樣一來，反而需要儘可能地開放出來你怎麼防守。而且在開放出來的前提底下，有所謂的 interoperability（共同性），不管是臺灣或者日本，跟理念相近的這些朋友一起訓練也非常重要。在所有這些部分上面，open source 都是一個前提，open source 反而增進 security。這個想法其實是本世紀的想法。上個世紀，連加密系統都被認為是軍事用途、不可以輸出的，在美國是這樣。但經過了三十年，整個資安界都瞭解到，我的密碼系統越公開，漏洞越早被發現，反而越好，也就越強健、更安全。所以接下來 AI 安全也會往這個方向走。

當然還是有互相競爭，但怎麼樣確保「安全」的這一部分，倒是可以分享出來的，它不會造成阻礙。因為在以前，你要對對方造成資安上的傷害，你也必須是一個大國才能做到，但現在情況不是這樣。就算只是一個三四個人的犯罪團伙，也可以做到不成比例的傷害，就像當初有人拿飛機去撞高樓一樣。所以在新的情勢底下，你要防範的就不是只有別的大國，而是各種小型犯罪集團。這些小型犯罪集團甚至已經可以透過勒索軟體、線上詐騙等等全自動化的方式去運作，連遙控都不需要。它犯罪得來的錢又可以再購入算力，形成持續迭代。所以未來可以想像這種情況更加複雜，所以各國也就更需要在「AI 如何防禦」這件事上面互相分享知識。

### 問：
另外，在教育場域上，有些人擔心未來是不是不需要老師，因為很多東西請 AI 就可以了，像語言翻譯、法律檢索等等都可以 AI 做。您覺得在教育現場，AI 應該怎麼被運用？

### 唐鳳：
不過這個挑戰，其實當初有搜尋引擎、有維基百科出來的時候就已經出現過一次了。以前是說老師腦裡有最專業的資訊，學生要聽老師講課。但就算沒有 AI，當時有了百科全書跟搜尋引擎以後，也已經出現：老師講到一半，學生打斷說「維基百科上不是這樣寫的，網上不是這樣講的」。也就是說，如果純粹只是知識性的傳授，有標準答案的那些東西，老師早就沒有壟斷的地位了。

所以我們在臺灣，2019 年就已經更改基本教育的教學方針。在以前，大家注重的是怎麼樣「有標準答案」，能夠理解到標準答案並回答出來；但從 2019 年開始，我們就改成說：沒有，因為這些有標準答案的東西，AI 做得不但比學生好，也比老師好。

我們要在學習過程裡面培養的，就不是那些標準答案，而是說「你怎麼激發自己的好奇心」——這個叫自發，「怎麼跟背景跟你不同的人互相合作」——這個叫互動，「怎麼在合作過程裡面不是我贏你就輸，而是一起看到雙贏、多贏的可能性」——這個叫共好。

自發、互動、共好，簡稱「自動好」。這些都是 AI 把有標準答案的事情都做完之後，人還可以產生的價值，也就是我們剛剛一直講的：人跟人之間透過交流所產生的意義。這個意義是建立在互相理解、互相關懷的概念上。而這個，我想對日本來說也不陌生，日本也是認為，一個人成功並不是只在於考試都考滿分，或者賺到最多錢，而是要跟社會保持羈絆、回應社會需求，能帶來整體價值。所以這點上，臺灣和日本是完全相同的。

所以我們的教育也不需要擔心 AI。如果我們請學生培養的都是「自發、互動、共好」，那老師就變成促進者的角色，讓學生跟學生之間更能夠透過互動來激發創意，而不是老師腦裡包含所有標準答案的那種模式——那個模式早就過去了。

我們在 2019 年推行新的教育方針之後，到了 2022 年，國中的學生第一批進入這樣的體制，去參加 ICCS（國際公民素養研究）評比。結果發現臺灣的學生，對於「自己能夠在環境永續、社會議題、人權等面向上貢獻」的公民素養跟自信，是全世界第一。那也讓大家比較放心的是，我們在 OECD PISA（閱讀、數學及科學能力）排名依然在前幾名，並沒有因為更注重公民素養就犧牲了 STEM 表現。所以我覺得這是最好的狀況。
