# 2025-09-05 AI 倫理加速：關懷六力

### Caroline Green：
哈囉，歡迎回到《AI 倫理加速》。我是牛津 AI 倫理研究院的 Caroline Green 博士。八月，我們與唐鳳大使進行了一場非凡的對話，探討「多元宇宙」——一種讓 AI 擴大人類合作、而非取而代之的願景。那集的迴響清楚表明，人們迫切需要有希望、有實踐性的替代方案，而不只是 AI 衝突或奇點的主流敘事。

今天，我們要從那個宏觀願景，走向具體的架構。唐鳳與我在牛津合作了一本新書，詳細說明我們如何建構這個未來。書名是《關懷六力：在 AI 治理中實踐仁工智慧》。唐鳳，歡迎回來。

### 唐鳳：
很高興再次回來，與你合作這個新微型網站真是令人愉快的一週。現在已經上線了。各位聽到這段話時，請去看看「6」，也就是數字「6」，6pack.care，閱讀我們的宣言和整個微型網站。

### Caroline Green：
與你合作是一大樂事，這集節目就是要向全世界介紹關懷六力（6-Pack of Care）和仁工智慧（Civic AI）的取徑。

### 唐鳳：
是的。我想，我從上一集 Podcast 收到的一個回應是：人們非常喜歡多元宇宙的能量——它是關於將衝突轉化為共創或共同生產的。但人們也想要一個非常具體、可操作的框架，可以放進他們的 AI 系統，或用來評估他們今天正在使用的 AI 系統，測試它是否真正尊重全人類。我想，這就是為什麼我們在關懷六力的應用倫理上，真的深入到了非常底層的工程細節。

### Caroline Green：
完全同意。這也是研究所加速學人計畫的核心——將哲學轉化為非常實用的東西，讓人們能夠在自己的生活中運用。

那麼，讓我們進入「公民關懷」這個核心概念——整個取徑和關懷六力都建立在這上面。告訴我這個具體的框架，特別是「關懷」——它通常被視為人與人之間非常私人的概念。但它如何應用於強大自主系統的治理？

### 唐鳳：
是的。當我們談到機器實現某些結果的能力時，大多數機器學習演算法只是針對那個結果進行最大化。例如，如果你訓練一個社群媒體演算法讓人們持續參與——也就是讓人黏著——那麼，如我們在上一集提到的，大多數演算法只會向你展示最具衝突性、最極端、最充滿仇恨的內容，因為這會觸動大腦那個想要不停爭論的部分，於是人們就黏在螢幕前。

另一方面，在最大化那個結果的同時，它也帶來了大量的負面外部性，污染了人與人之間的關係。我們以前在社群媒體、部落格等地方追蹤同樣的人——我們可以有共同的基礎來交談。但現在，為了參與度而最佳化之後，每個人都活在自己的世界裡，社會結構被嚴重掏空，再也沒有共同的基礎可以談了。

那麼，我們如何描述一類 AI 演算法——它不只是針對特定結果進行最佳化，而是針對更接近社會健康的東西？我研究了各種不同的倫理傳統，發現有一個叫做「美德倫理」的傳統，它想要在一個行動者——人或機器——身上灌輸某些特質、某些屬性。而「關懷」就是讓我們以受那些演算法影響的人的實際需求為速度來傾聽的屬性。所以，不是設計者決定一切，關懷告訴我們，這必須從受影響的人的需求出發。而「公民關懷」意味著，它不只是個人的需求，還包括人與人之間健康關係的需求。

### Caroline Green：
好，這對我來說非常有趣；如我在上一集告訴你的，我的研究背景是社會關懷——指的是支持患有疾病或殘疾的人在日常生活中活動的實踐。但它有很多面向——它真的是在許多不同層面上，從實際上和情感上支持人們的生活。在社會關懷領域——在英國，使用關懷服務的人、提供關懷的人等等——他們想讓每個人的政策環境變得更好，最近出現了「共同生產」（co-production）這個新概念，讓我想起你剛才說的。

所以，共同生產真的是一種心態，一種一起工作的方式——不同的行動者聚在一起，找到共同基礎，然後共創和共同生產解決方案。它的誕生是因為，通常有一些群體——特別是使用關懷服務和提供關懷的人——權力較小；他們的聲音不像更有權力的群體（如服務提供商、政策制定者等）那樣被聽見。所以，共同生產就是讓人們聚在一起，讓他們互相傾聽，然後創造出適合彼此需求的解決方案。

它是一種挑戰權力結構的心態，然後付諸實踐，創造出不僅非常符合人們需求和意願，而且還能創造社群感和關係健康的東西。我在思考，從我的脈絡來看，它如何以非常實際的方式，與公民關懷取徑銜接。

### 唐鳳：
確實。我想「公民」這個詞在這裡意味著，我們把受影響的人有能力讓自己參與決策過程，視為主要的結果，而不是最大化利潤、參與度或諸如此類的東西。如果有什麼是我們必須最佳化的，那一定是公民肌力——人們有能力以團結的方式行動，共同決定、共同生產未來，以及 AI 機器和供應商等提供的服務也是這個未來的一部分，但不能主宰這個未來。

### Caroline Green：
很好。那麼，回到社會關懷的例子——社會關懷與 AI，這已經是現實了。人們在日常生活中使用 AI 系統，無論是在自己家裡需要關懷和支持的人，還是安養院的照護提供者，我們都看到這些 AI 系統。

但我認為，目前在許多人之間有一種擔憂；我們在社會關懷工作中看到，這些 AI 系統並不真正符合人們的現實，它們不一定能滿足他們的需求。人們有一種需要和願望，想要保持關懷的關係性。人們不想讓機器人接管關懷，但他們希望 AI 和科技能幫助他們過上更好的生活，幫助他們更好地關懷。

所以，我看到公民關懷取徑在這裡確實很有幫助，確保我們能充分利用 AI 的力量來促進關懷的關係性，在人們的關懷情況中幫助他們，同時也真正建立符合人們需求的系統。

### 唐鳳：
是的，我認為倫理學對我來說，是關於辨別好系統和壞系統、道德上正確的系統和道德上錯誤的系統。但許多倫理傳統——那些從結果、從成果出發的——當我們面對大量的權力不對稱時，開始產生大量的副作用。而權力不對稱，正是我們與 AI 系統互動時的定義性特徵。

現在，想與治療師或教練交談的人，發現 AI 聊天機器人每單位時間能產生一萬倍的文字，而成本只有幾分之一。而且它們聽起來都非常令人信服。但它真的提供了關懷嗎？還是只是依賴和奉承？如果只測量結果，就很難說——它能產生多少字的支持，人們在看了聊天機器人的對話之後自我報告感覺有多好？

所以，我們需要別的東西。在機器行動者和人類行動者之間相差千倍的速度下，傳統倫理學有時會失效。但有一個關懷的框架，從一開始就預設了這種不對稱，因為 Joan Tronto——研究「公民關懷」或「共同關懷」的人——她的想法是，一個好的園丁必須依著花園的節奏，以花園生長的速度來耕耘。這就是關懷六力的根本：讓照顧人們彼此關係的機器人，以那些關係的速度來運作。

### Caroline Green：
好。有一件事我真的很想和你深入探討，那就是「公民關懷取徑」在 AI 倫理中，如何挑戰我們目前看待 AI 及其在我們生活中的角色的方式。過去一週與你一起工作，看到你如何將 AI 融入自己的生活，讓我大開眼界。

而「共同在場」（co-presence）的想法——AI 現在在桌上有了一席之地——我覺得非常有趣。你在自己的工作中挑戰了實體存在的規範。你能告訴我們你在 2017 年日內瓦聯合國網際網路治理論壇的經歷嗎？你當時是透過一個遠端存在機器人參與的？

### 唐鳳：
是的。2017 年，我受邀在聯合國日內瓦網際網路治理論壇發言，討論如何賦權小島嶼和內陸國家——那些網路並非理所當然的地方——的人民。這是臺灣擁有豐富經驗的領域，但因為臺灣目前不是聯合國成員，在聯合國日內瓦大樓門口，如果你出示臺灣護照，你就進不去。這是一個典型的「關於我們，卻沒有我們」的案例。所以，恰好 Double Robotics——一家製造這些機器替身的公司——他們的機器不需要護照就能進門。

我當時在臺北，只是遠端操控那台機器。當有人舉手，我可以轉動機器面向他們。這成為 1971 年以來，第一次臺灣代表和北京代表在同一個聯合國會議上共同發言的場合。

這創造了一個外交先例，我們之後不斷引用，也讓更多的參與成為可能。所以我認為，共同在場是關於找出誰可能系統性地、結構性地缺席——因為一些奇怪的政治或地緣政治原因——然後找到讓智慧體重新呈現他們實際需求和想法的方式，而不只是讓別人代表他們。這種「重新在場」、「共同在場」，對於共同生產非常重要。

而現在在我的生活中，我一直在使用 AI 智慧體。事實上，在我們交談的同時，我們有這個 AI 智慧體運行在一台本地電腦上——這叫做 Daylight Computer。但我認為，我對此的思考方式是，它們都是對關係健康的輔助。這台電腦沒有相機，所以我不會拍照。它甚至沒有顏色，所以我周圍的現實總是比這個螢幕更生動。它只是非常平靜地在背景中運作，但如果我確實需要一些提醒、查找資料等，那麼它也有我所聽到和所想的具體情境，以便能夠提供即時的協助。

### Caroline Green：
我確實需要這樣的機器人。「公民關懷取徑」在在場方面，是關於挑戰那些可能不被聽見或被隱藏的人的聲音太難提升這個想法。現在有很多方法可以做到這一點，真正強化關係健康。

### 唐鳳：
是的。當我擔任臺灣數位發展部長時，我常說我接受所有不同的立場。所以，如果在多方利益相關者的對話中，有一部分我不理解——無論是因為我沒有親身經歷，還是因為我看問題的方式不同，或者只是因為我不說那種語言——那是我的錯，不是那些利益相關者的錯。所以，我發現輔助性智能，或用於公民關懷的 AI，在這方面真的很有幫助。首先，它可以幫助歸納和總結我們談論的內容，也可以跨文化翻譯，使得即使在我真的沒有親身經歷的地方，現在的語言模型也能找到幫助我理解的比喻。

所以，從我個人有過的經歷重新講述他們的故事——現在人們已經將這種方法應用到了所謂的「意義構建工具」（sense-making tool），這樣你就可以有大約十個人圍坐在桌子旁，談論他們的親身經歷。幾乎是即時的，它能夠梳理那段對話，並反映給人們，讓沒有相同親身經歷的人也能理解他們在說什麼。這樣做，不是為了取代人與人之間的聯繫，而是增強我們的公民肌力，確保我們能夠跨越更長的文化距離來理解和更有同理心。

### Caroline Green：
好。現在讓我們深入書的框架——關懷六力：覺察力、負責力、勝任力、回應力、團結力、共生力。

讓我們從基礎開始——覺察力。你寫道：「在最佳化任何事情之前，我們選擇要注意什麼。」為什麼這是至關重要的第一步？

### 唐鳳：
是的，我將有覺察力的 AI 想像成一個對實際需求感到好奇的 AI 系統。它不是從為某些東西（比如參與度分數）最佳化開始，而是首先問：當地的情境是什麼？邊緣化的群體是誰？他們的生活是什麼樣的？所以，它不是預先規定任何事情，而是使用我們談到的方法與社群互動——橋接地圖、不尋常的共同點、意義構建的想法——所有這些都是關於創造群體自拍，而不只是統計平均值。

所以，在線上社群媒體的情境中，現在越來越多的社群媒體嘗試「社群備註」（community notes）——不只是顯示引發憤怒的最瘋傳帖子，而是眾包最有幫助的當地情境，附在瘋傳帖子上。最瘋傳的帖子現在附有「不尋常的共同點」——通常不同意的人設法同意某些備註是有幫助的。而這種橋接地圖是一個更準確的群體自拍，因為它顯示了不同群體之間存在的差異、分裂我們的東西，但也顯示了儘管有分裂但統一我們的東西。

所以，覺察力是同時針對各個不同子群體的獨特需求，以及那些群體本身可能不知道的橋樑——但在關注那些共同需求之後，它們都意識到：這些是共同的需求，如果 AI 系統與我們一起克服這些問題，它就不會侵犯我們也認為神聖且 AI 系統不應干擾的重要事情，比如人類尊嚴。

### Caroline Green：
所以，當我們確實在傾聽，但之後沒有採取行動，你說那只是作秀，對吧？你是否認為，一旦注意到一個需求，下一步——負責力——就是我們如何從認識問題到做出可驗證的承諾？

### 唐鳳：
確實。現在，當人們在開發 AI 時，許多前沿實驗室都有這個叫做「模型規格」（model specification）或「模型規格書」（model spec）的東西。這樣的規格書是一種向相關社群公開承諾的方式，關於這個 AI 系統試圖做什麼——以及它承諾不做什麼。

幾年前，我們引入了「對齊議會」（alignment assemblies）的想法，其中一個統計上具代表性的公眾切片聚在一起，在小組中，他們查看彼此對 AI 系統行為準則的建議，將這些整合到自己的親身經歷中，並對其進行投票支持或反對。經過幾輪審議，他們確定了這些關於 AI 系統在其社群中應該和不應該做什麼的橋接共識。

如果我們能將那些有覺察力的結果納入供應商做出的承諾，那些供應商就可以被稱為負責任的 AI 供應商，因為他們對履行那些對齊議會共同生產中他們能做到的部分負責——同時也劃定了非常清晰的界限，說明我們只是履行這麼多，超出這個範圍就不在我們的職責之內了。

### Caroline Green：
回到我聽你解釋關懷六力的時候，我總是回想起我之前提到的，我在過去兩年一直在做的工作——試圖通過共同生產的過程找出 AI 在社會關懷中負責任使用的含義。

我們在過去兩年所做的，是把來自關懷社群的人——使用關懷的人、提供關懷的人、關懷服務提供商、政策制定者——聚在一起，找到共同基礎，互相交談，共同解讀。在這個過程中，負責力是關於不同群體的人真正找到自己在責任格局中的位置，以及對他們而言負責任意味著什麼。我們在這個共同生產中也有科技提供商——專門為照護工作建立 AI 系統的人。他們說，好吧，我們真的想向人們展示我們是負責任的。所以，根據我們對 AI 在社會關懷中負責任使用的定義——關於支持關懷的價值觀，而不是破壞或損害關懷和人權的價值觀——他們說：是的，這就是我們想做的，但我們實際上需要思考這對我們在實踐中意味著什麼。所以他們制定了一個承諾書。現在，科技提供商正在簽署這份承諾書，不僅僅是簽署——他們還需要對此負責。

### 唐鳳：
是的。我認為你描述的正是正確的流程，其中承諾不只包括承諾的範圍，還包括真實的時間線——我們將在這個時間前交付這個——護欄、補救措施、問責機制，以及希望還有獨立監督。而我剛才描述的公民關懷機制，更多是關於自動化翻譯。所以，不只是人們聚在一起交談，而是在即時狀態下，他們從有覺察力的 AI 那裡看到一個橋接地圖，它把這些反映給他們，而不需要等幾個月讓人類意義構建者來編碼這些。

一旦完成，我們就把那些非常清晰的規格書，轉化成 AI 系統已經能夠理解的東西。所以，不是人類程式設計師把那些承諾中的每一項都編入他們的系統邏輯，而是它正在成為一個課程，這樣當 AI 系統在這些資料和材料上進行訓練時，進入的資料已經通過這些承諾的稜鏡過濾——這樣，一旦 AI 訓練完成，就能直觀地理解對這群人來說什麼是道德的。

### Caroline Green：
有一個問題我迫不及待想問你。我們如何重新設想 AI 在社會關懷中的共同生產過程——那個主要是在沒有 AI 的情況下發生的——我們現在如何用 AI 來做這件事？

### 唐鳳：
是的。所以，一個非常快速的介紹是——因為你確實有所有那些審議式民調序列的文字記錄——不只是採取最終投票的量化資料，我們還可以採取那段對話的質性資料，並將其輸入意義構建工具（這是開源的）。意義構建工具就能夠對人們在審議結束時的投票進行注解，並將那些決定細化，例如：人們同意這些是重要的原則，但人們開始說：在部署時，你需要注意這個，注意那個。

所以，當這些提供給那些供應商時，你會生成一個更豐富、更厚實的資料集。他們仍然遵循從審議式民調投票中得出的同樣承諾。但之後，當他們訓練他們的 AI 系統時，這些系統還可以從審議過程本身提供的豐富情境中學習。我很想介紹意義構建和橋接地圖的使用——更重要的是，把這些反映給人們，這樣人們也許可以和一個關懷助手交談，問在之前的審議中達成了什麼共識，而現在我有這個真實的情況，你如何與那個進行比較？

### Caroline Green：
好的，是的，我對在這個過程中使用史丹佛 AI 審議平台的下一步感到非常興奮。

讓我們繼續下一力——第三力，勝任力。這裡真的是工程與倫理的相遇。告訴我們更多關於勝任力的事。

### 唐鳳：
勝任力對我來說意味著賦能接受關懷的人，不只是解決他們的需求，還要確保他們的關係也能蓬勃發展。所以，例如，如果你想服務人們在學習方面的需求，就不是引入一個可以為他們完成所有作業的 AI。一個勝任的老師不只是為學生解決所有作業，而是以學生的速度工作，確保學生對這些學科、這些不同的領域感到好奇，並將學生介紹給更多的知識和實踐社群，為基於同儕的學習介紹更多同儕，為基於目的的學習介紹更多目的，等等。

一個勝任的老師，或者園丁，是一個非常好的勝任照顧者的比喻。當 AI 處於這種情況時，我們非常重要的是要看看 AI 是否在帶走現有的人際關係——因為人們越來越依賴 AI——還是它在鼓勵人類建立更多其他人與人之間的關係。

### Caroline Green：
這讓我想起了我最近遇到的另一個例子——那些應該支持殘疾人士或體弱老人，或與失智症各個階段共存的人，支持他們在自己家中生活更長時間的系統。我在這裡看到的是，這些系統背後的意圖是好的，一般來說人們非常興奮，因為人們想盡可能長時間在自己的家中獨立生活。但具體來說——我想到一個案例，有人某天向他們的 AI 尋求幫助記住某事，系統幫得很好。但另一天，那個人摔倒了，有人在那裡支持他們。那個支持者試圖通過 AI 獲得一些幫助：我怎麼幫助一個人站起來？AI 告訴他們，哦，你應該叫救護車。所以他們叫了救護車，但沒有救護車來，因為緊急服務不足，尤其是在冬季。

所以，這裡，有一天這個系統真的幫助了那個人，但另一天，在另一個情境中，它並不是很有幫助。人們擔心 AI 可能意味著他們會與他們和來家裡的照護者的重要關係切斷——因為現在的政策是：哦，我們只需要照護者每天來一次，而不是兩次，因為有一個 AI 系統在支持。那麼，真正思考這些 AI 系統如何在人們的情境和情況中運作，有多重要？

### 唐鳳：
是的，確實。我認為這是與以前的「來自人類反饋的強化學習」時代的根本區別——ChatGPT 就是在那個時代誕生的——因為他們通過讓個人對各種可能的回應進行排名來對各種可能的結果進行排名，這個回應比那個回應好。一旦他們讓大量的人對好的和壞的回應進行標記，它就產生了一些會持續得到人類標記者好評的東西——但它也讓 ChatGPT 極度奉承、極度諂媚，因為大多數人在這種個人二元關係中，他們會最佳化短期滿足感。所以，如果有什麼在奉承我，我當然會讓它通過。但如果有什麼真的在檢查我的理解，真的在反駁我的幻覺，也許我會給它一個踩。對吧？所以，從個人人類反饋中訓練的 AI，可能不會優先考慮嵌入在社群中的那個人的關係健康。

所以，現在我們看到多智慧體設置中有更多進展，智慧體不是從個人反饋中學習，而是從社群反饋中學習——所以你描述的情況，那個人不只是獨自在家，而是有一個幫助者的社群，將這些文化、家庭等關係優先化為主要的分析單位，然後進行強化學習以獲得更好的分數，不是來自豎起大拇指或踩，而是來自那些社群的相互認可和相互保證。這是一個非常活躍的多智慧體研究領域。

### Caroline Green：
是的，我也可以在這裡看到，我們需要採用一種建立系統的心態，這種心態是關於人們的需求，並從他們的需求和願望中學習，而不是基於假設——例如，特定群體的假設。所以，現在有一個越來越多人感興趣的社群，他們對專門為老年人建立系統感興趣。但我們沒有一個所有人都一樣的老年人群體。如果我們想建立能夠幫助老年人在他們情況中的系統，我們應該更多地考慮那些在地的、個體的需求、家庭健康、關係健康等等。

### 唐鳳：
完全正確。比如說，如果我是一個非常老的人，但我沒有這個特定的虛弱，一個 AI 系統只是假設我有並提供各種幫助，這真的在剝奪我的能動性——這真的是一種歧視。所以，要克服那些非常簡單、非常薄弱的標籤和刻板印象，正是為什麼我們需要這種社群 AI 對齊，因為這是提供來自那個特定社群的豐富情境的唯一方式。

### Caroline Green：
讓我們繼續下一力。一旦我們知道一個系統並不完美——而沒有系統是完美的——它就需要改變，需要調整。我們如何設計在社會速度下可糾正和可調整的系統？

### 唐鳳：
我認為最重要的事情是賦能最接近痛苦的人。也就是說，正在遭受傷害的人應該是定義傷害的人。所以，不只是設計者試圖弄清楚如何確保它不傷害任何人——這根本不可能；所有未預見的後果總有一天會傷害某人——我們想要的是，人們不要以「這就是科技的工作方式」來減輕那些傷害。

這裡的想法是，我們需要創造類似維基百科的東西來評估那些傷害。所以，我工作的集體智慧計畫（Collective Intelligence Project）有這個叫做 weval.org 的網站，社群可以在那裡為傷害撰寫在地化的基準。所以你會在那裡找到，例如，斯里蘭卡的人們眾包對他們在當地情境中重要的事情，這樣聊天機器人就能理解對那個社群重要的是什麼。有些社群正在遭受精神病、心理健康問題，他們想確保 AI 聊天機器人不會加劇這些問題。

所以，這就閉合了循環——因為一旦那些個別的傷害被提到前台，並被做成每個人都可以看到的東西，我們就可以持續地對前沿模型運行那些基準，這樣當它們發布新版本時，它不只是在解決國際奧林匹克數學競賽的計分板上競爭，還要確保它在那些真正導致人們傷害的基準上沒有退步。

### Caroline Green：
好的，是的——這實際上是我們在負責任 AI 和社會關懷計畫中的下一步，要建立一個資料庫，人們可以在那裡報告他們的經歷，無論是傷害還是他們覺得有幫助的。所以我認為我們在這裡需要一個 Weval。

### 唐鳳：
是的，我們在這裡確實需要一個 Weval，這樣當那些技術的下一個版本出來時，你可以在模擬環境中——一個模擬的處於完全相同情況的人——然後評估那些機器人的下一個版本將如何回應，確保它們持續運行這個測試，以便在第二個版本發布到世界上時，它們不會造成更多傷害。

### Caroline Green：
好的，讓我們繼續最後兩力，它們真正地看待了公民關懷取徑需要的生態系統。團結力是第五力，它是關於讓合作成為阻力最小的路徑。我們如何建立這個基礎設施來對抗壟斷權力的趨勢？

### 唐鳳：
是的，所以前四力形成了一個循環，對吧？你對群體的需求有覺察力。你回應那些需求。你有勝任力，所以你交付並滿足其中一些需求。在滿足它的過程中，你也許造成了一些傷害，然後非常快地轉向，以有回應力的方式回到覺察力。

但有一件事是，如果權力不對稱太大——如果我們沒有選擇從科技供應商那裡接受關懷，如果沒有辦法切換到其他供應商，沒有辦法把我們的資料從這次對話帶到另一個——那麼壟斷性的提供商就根本沒有動力繼續有覺察力。這一次又一次地發生：當一個服務很小時，比如當它是一個新興的社群媒體平台時，它通常非常有覺察力、非常有回應力。但在某些時候，當大多數人口都在那個平台上時，它突然不再關心你了。這不是單靠工程就能克服的東西——通常需要政策。

在這裡，人們已經談論了很多「互通性」（interoperability），對於大型科技系統。試想，如果沒有號碼可攜性的規定，新的電信公司根本不可能進入市場，因為從一個供應商切換到另一個供應商太不方便了。所以，現在我們看到政策制定者——例如，在猶他州——要求從明年七月開始，你可以從一個社群網路切換到另一個，並帶走你所有現有的聯絡人。

我們還需要選擇性揭露或「部分匿名」（meronymity）的想法——部分匿名——因為分享我們親身經歷的一部分，是為了指出那些技術造成的傷害。但如果我們必須用真實姓名簽署每一份報告，我們基本上是在人肉自己。大多數處於脆弱情況的人，不會願意貢獻他們的親身經歷來糾正那些技術，只是因為他們不信任處理這些資料的人。

但如果我們使用部分匿名，選擇性揭露，那麼我們可以確保它來自聲稱是這個國家居民的人，聲稱在 18 到 50 歲之間——但那時候你不需要再多了解任何事情。互通性和部分匿名一起為代理性 AI 提供了良好的基礎設施，這樣我們就可以知道每個聊天機器人、每個技術來自某種供應商，它簽署了什麼樣的承諾，這個承諾進入了哪個社群——而不需要揭露參與審議或維基評測的任何社群成員的私人細節。

### Caroline Green：
那肯定也會激勵提供商確保他們的系統能正常運作——讓他們有勝任力？

### 唐鳳：
向上競爭，而不是向大腦底部競爭。

### Caroline Green：
所以，最後一力是共生力。它直接挑戰了奇點的敘事。你引入了日本神道中一個非常有力的比喻——地神（kami）。在 AI 的情境中，地神是什麼，為什麼這個願景比奇點的敘事更可取？

### 唐鳳：
是的，地神在這裡被比喻性地使用，作為一個在地的管理者——具體來說，是一個被限定在某個特定場所或社群的 AI 系統，它與關係健康合作。所以，它所關心的只是這個特定社群的關係健康。我們談到社群節點如何能把一個社群橋接在一起。起草這樣社群節點的 AI 就是這種地神。它根本不擔心用迴紋針或太陽能板鋪滿整個銀河系，或任何類似的東西。它沒有試圖自己奪取權力。它所關心的只是這個社群的關係健康。如果社群完全康復了，如果社群已經向前走了，如果不再有這樣的社群了，那麼地神可以毫無遺憾地被關閉。

所以，在倫理術語上，這意味著它針對關係公民關懷的美德進行最佳化。它不是針對宇宙中最大幸福的結果進行最佳化。這是我們稱之為「多元宇宙」（Plurality）的水平替代方案——許多在地的地神，以重疊的方式，照顧每個花園，而不是一個垂直的、不知道「足夠」是什麼的超智能最高園丁。這種「夠了就好」、這種有界限性，是共生力的定義性特徵。否則，它就是寄生性的。

### Caroline Green：
所以，在我們開始合作之前，研究所創建了一個叫做 Dedicate 的關懷助手，它根植於與家庭照護者的許多對話，以了解他們需要什麼才能在日常生活中作為照護者得到支持。Dedicate 背後的想法，很好地對應了具體的共生力，因為它完全是關於創造聯繫——使用 AI 在組織和人們之間創造聯繫，以支持家庭照護者獲得建議和資訊。我們建立了一個系統，其中這些內容是由他們提供給我們的，然後在其之上有一個 AI，讓家庭照護者在最需要時能夠方便地獲取資訊。但那個聊天機器人也有它的限制。所以，如果有人輸入一個非常具體的醫療問題，或者完全不在照護範圍內的事情，那個聊天機器人會說：哦，這不是我的領域。

### 唐鳳：
是的，我試過了。我去了 dedicate.life，問說我應該帶什麼設備去錄 Podcast。然後它說它不知道。

### Caroline Green：
如果它給了你答案，我會很驚訝的。但我真的很高興發現 Dedicate 及其背後的想法與公民關懷取徑如此一致——我們著手的方式，提升人們的聲音來聽取他們的需求，然後建立一個系統，把他們與世界上現有的東西聯繫起來。所以他們實際上也感到：哦，我在這件事上並不孤單——而地神確實在那裡發揮了作用，因為它是關於那個特定的社群。

而 Dedicate 現在是為生活在英國，或者甚至具體地說是英格蘭的人。但我們可以在其他情境和文化中建立這樣的系統，在那些地方，照護可能是不同的。

### 唐鳳：
是的。所以它形成了一個地神的聯邦，而不是試圖建立一個知道一切、能做一切的超級智能。通常，當你試圖建立一個在任何地方都有效的超級解決方案時，它最終只是強迫每個地方看起來就像設計者居住的地方——然後就殖民了世界的其他地方。從共生力出發意味著知道這種有界限性、這種「夠了就好」，實際上對人們的關係健康更好。

### Caroline Green：
所以，唐鳳，我們從公民關懷取徑中學到了很多。我們了解了框架——關懷六力，它已經準備好被使用——實際上，它已經在各種情境中被使用和測試了。所以，它是一個供人們使用的工具。

我認為這個取徑確實很緊迫。我也研究權利——人權和老年人。有一個真正令人興奮的新進程，在聯合國內部朝向關於老年人權利的潛在新公約邁出了有力的一步。與你合作，我對現在使用 AI 的可能性感到非常興奮，讓各個年齡段的人——因為我們都在老化，老化是每個人類的事情，而不只是老年人——也提升世界各地老年人的聲音，來告知這個在聯合國內部的進程。

你在哪裡看到緊迫性，你會告訴我們的聽眾，在他們聽完我們的 Podcast 並更多了解公民關懷取徑後，現在應該做什麼？

### 唐鳳：
首先，去看看我們的微型網站 6pack.care，那裡連結到我們的宣言以及可以開始使用的具體工具。任何人都可以舉辦一個在地的對齊議會；任何人都可以使用那個對齊議會的結果調整一個在地的 AI 模型，並親自看看這種社群模式是否能更好地回應他們社群的需求。任何人都可以倡導社交可攜性、提供商之間的互通性——這些都不是理論性的。

在前沿實驗室——我們與 Google 的一些部分合作相當密切，例如 Jigsaw，我剛在 DeepMind 做了一場演講。微軟研究院的一些部分在協作實驗室專注於多元宇宙研究。OpenAI 和 Anthropic 都與 CIP 共同舉辦了對齊議會。

所以，我認為前沿實驗室不是不知道這些方法。只是政策制定者現在需要了解這不是科幻小說。這是每個實驗室在其研究能力方面已經進行了相當多次迭代的東西。所以，現在政策制定者的要求必須是讓這成為基準。如果一個 AI 產品不具有覺察力，那它就是壞掉的。如果一個 AI 產品沒有回應力，那它就是壞掉的。如果整個 AI 智慧體生態系統不是以團結的方式運作，而是在互相競爭消滅對方，那也不是最好的想法。所以，將共生力和團結力作為我們科技政策的基石——這非常重要。所以，我認為我們有這個 Podcast 是很好的，我們有這個微型網站可以將政策制定者、工程師、研究人員和受 AI 影響的人，引導到這個既是哲學、也是政策和工程的共同詞彙。

### Caroline Green：
而這個取徑給了我很大希望的是，我們可以在一個看起來往往太兩極化的世界中，再次建立更好的關係——在那裡，我們不以真正讓我們作為人類、關係性存在感到快樂的方式，互相合作、互相協作、互相陪伴。

### 唐鳳：
是的，確實。這種公民關懷的缺失表現為兩極化。而這種感覺——我們都很兩極化，我們無法採取協調行動，人們在社群媒體上互相仇恨——就像一個巨大的網，覆蓋整棵樹，使樹無法接受陽光。我認為是時候走出被兩極化的社群媒體造成的十年長幻覺，真正嘗試公民關懷了。

### Caroline Green：
是的，非常感謝你，唐鳳，感謝這次 Podcast，感謝介紹公民關懷的架構。它真的是一個非常引人注目且非常必要的願景。本著關係健康的精神，如你所提到的，任何人——所有聆聽者，接觸到這項工作的人——都非常歡迎透過我們的微型網站和關懷六力與我們互動。我們也會確保人們有那個連結。

再次感謝你。與你合作真是太好了。

### 唐鳳：
是的，我也有同感。願生生不息，繁榮昌盛。
