# 2025-11-06 開放 AI 安全，巨頭迎來轉折點

### 唐鳳：

上週，OpenAI 釋出全球首次開放的安全推理模型：「gpt-oss-safeguard」。雖然不像每次 ChatGPT 推出新一代模型那般轟動，但它其實事關重大，能真正影響 AI 內容服務是否能持續受社會信任。

這是因為近期屢傳 AI 模型不當運作的悲劇事件：例如，美國青少年與 AI 聊天互動後輕生、也有許多人以此產出大量兒少不宜的內容。這已經促使澳洲在今年底，禁止未滿十六歲兒少使用社群平台，就連 AI 聊天機器人也受限制；同時間，美國參議院也正在研議類似法案。

這讓 AI 巨頭們意識到，如果缺乏應對安全政策的能力，整個產業恐怕會面臨被 全面監管的命運。

今年二月在巴黎 AI 峰會上，我與 Meta 首席 AI 科學家 Yann LeCun、谷歌前執行長 Eric Schmidt 協力啟動的 ROOST 基金會，致力與 OpenAI 等巨頭合作，研發開放授權的安全推理模型。

我們採用的是在 ChatGPT 與 Sora 等每天實際運作的系統，不但具備先進的語意理解能力，而且善於面對各式鑽漏洞行為。歷經八個月研發，這個開放模型的釋出，意味著未來 AI 內容的審核，能從「黑箱」走向透明。

運用這個模型時，首先要輸入安全政策（policy），包括在地法律、組織規範、社會文化。接著，輸入待分類的內容。最後，這個模型產生完整「思路」：它會判斷這是否符合安全政策，並解釋它推理的過程，OpenAI 不會有任何預設立場。

其中值得關注的，是它能因地制宜。例如在泰國，冒犯君王屬於違法內容，但在其他國家的外國公民，就可以自由談論。

當推理過程能被公開稽核，不但不至於屢傳誤判，安全邊界也不再由矽谷單方面決定，而能由各地社群根據自身脈絡，隨時調整安全政策。

而由於任何組織都能直接採用、修改與部署自有安全系統，這也解決了以往中小型平台的困境：常因為單靠人工審核成本高昂，導致兒少暴露在風險之中，或是讓違法內容任意流通。

這套模型，讓上述種種困境不再是理由；當過去認知的風險，轉化為可公開驗證的安全機制，才能避免整個產業因少數不合規範者，受到牽連，甚至發生「全面禁止兒少使用 AI」等極端措施。

這也顯示，內容安全正從過去的「軍備競賽」和「封閉系統」，轉向透明、協作。更加多元開放的 AI 未來，正在加速到來。

> (採訪及整理：游羽棠。授權：<a href="https://creativecommons.org/licenses/by/4.0/deed.zh-hant">CC BY 4.0</a>)
